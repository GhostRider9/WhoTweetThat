{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tain_norm.txt exists. Will load it\n",
      "test_norm.txt exists. Will load it\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_file_exist(file_path):\n",
    "    my_file = Path(file_path)\n",
    "    if my_file.is_file():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_fwf(data_folder + \"train_tweets.txt\", infer_nrows=150, sep=\"\\t\", header=None,\n",
    "                        names=[\"UID\", \"Twitter\"])\n",
    "    train = train.dropna()\n",
    "    test = pd.read_fwf(data_folder + \"test_tweets_unlabeled.txt\", infer_nrows=150, header=None, names=[\"Twitter\"])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def regularize(text, tk):\n",
    "    tokens = tk.tokenize(text)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.find(\"http\") != -1:\n",
    "            token = re.sub(r\"http://\", \"{\",token)\n",
    "            token = re.sub(r\"/[\\w./-]+\", \"}\", token)\n",
    "            tokens[i]=token\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stemming(tokens, stemmer):\n",
    "    return [stemmer.stem(token).lower() for token in tokens]\n",
    "\n",
    "\n",
    "def pre_processing(df, description):\n",
    "    stemmed=[]\n",
    "    tk = TweetTokenizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    print(\"Start normalizing {} set\".format(description))\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        tokens = regularize(row[\"Twitter\"], tk)\n",
    "        stemmed.append(\"\\x01\".join(stemming(tokens, stemmer)))\n",
    "    del df[\"Twitter\"]\n",
    "    df[\"Twitter\"] = stemmed\n",
    "    df.to_csv(data_folder + \"{}_norm.csv\".format(description),index=False)\n",
    "    return df\n",
    "\n",
    "# Specify data folder\n",
    "data_folder = \"/home/zlp/data/SML/\"\n",
    "\n",
    "train, test = read_data()\n",
    "if check_file_exist(data_folder+\"train_norm.csv\"):\n",
    "    print(\"tain_norm.txt exists. Will load it\")\n",
    "    train_norm = pd.read_csv(data_folder+\"train_norm.csv\")\n",
    "else:\n",
    "    train_norm = pre_processing(train, \"train\")\n",
    "if check_file_exist(data_folder+\"test_norm.csv\"):\n",
    "    print(\"test_norm.txt exists. Will load it\")\n",
    "    test_norm = pd.read_csv(data_folder+\"test_norm.csv\")\n",
    "else:\n",
    "    test_norm = pre_processing(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize data using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=328931), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35437), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_data(feature_extractor):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    training_classifications = []\n",
    "    for _, row in tqdm(train_norm.iterrows(),total = train_norm.shape[0]):\n",
    "        feature_dict = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))   \n",
    "        training_set.append(feature_dict)\n",
    "        training_classifications.append(row[\"UID\"])\n",
    "    for _, row in tqdm(test_norm.iterrows(),total = test_norm.shape[0]):\n",
    "        features = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))\n",
    "        test_set.append(features)\n",
    "    vectorizer = DictVectorizer()\n",
    "    training_data = vectorizer.fit_transform(training_set)\n",
    "    test_data = vectorizer.transform(test_set)\n",
    "    return training_data,training_classifications,test_data\n",
    "\n",
    "trn_data,trn_classes,test_data = prepare_data(get_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [KNeighborsClassifier(n_jobs=-1),DecisionTreeClassifier(),RandomForestClassifier(n_jobs=-1, n_estimators=50),\n",
    "        MultinomialNB(),LinearSVC(),LogisticRegression(n_jobs=-1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just for comparing the perforamnce of different models\n",
    "High memory requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from time import time\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        s_time = time()\n",
    "        predictions = model_selection.cross_val_predict(clf, data, classifications, cv=10)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        print(\"time cost:{}\".format(time() - s_time))\n",
    "\n",
    "#do_multiple_10foldcrossvalidation(clfs,trn_data,trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost:8655.274057388306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zlp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/zlp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/zlp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save_predicted(predicted, index):\n",
    "    output = [(i+1,pred) for i,pred in enumerate(predicted)]\n",
    "    out_df = pd.DataFrame(output, columns=[\"Id\", \"Predicted\"]).set_index(\"Id\")\n",
    "    out_df.to_csv(data_folder + \"predicted_{}.csv\".format(index))\n",
    "\n",
    "def fit_predict(clfs, indexs, data, classifications):\n",
    "    for i in indexs:\n",
    "        s_time = time()\n",
    "        clfs[i].fit(data, classifications)\n",
    "        save_predicted(clfs[i].predict(test_data), i)\n",
    "        with open('trained_{}.pkl'.format(i), 'wb') as fid:\n",
    "            pickle.dump(clfs[i], fid, protocol=4)\n",
    "        print(\"time cost:{}\".format(time() - s_time))\n",
    "    \n",
    "def load_predict(indexs):\n",
    "    for i in indexs:\n",
    "        with open('trained_{}.pkl'.format(i), 'rb') as fid:\n",
    "            model = pickle.load(fid)\n",
    "        save_predicted(model.predict(test_data),i)\n",
    "\n",
    "models_index = list(range(4,5))\n",
    "#fit_predict(clfs, models_index, trn_data, trn_classes)\n",
    "load_predict(models_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
