{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tain_norm.txt exists. Will load it\n",
      "test_norm.txt exists. Will load it\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_file_exist(file_path):\n",
    "    my_file = Path(file_path)\n",
    "    if my_file.is_file():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_fwf(data_folder + \"train_tweets.txt\", infer_nrows=150, sep=\"\\t\", header=None,\n",
    "                        names=[\"UID\", \"Twitter\"])\n",
    "    train = train.dropna()\n",
    "    test = pd.read_fwf(data_folder + \"test_tweets_unlabeled.txt\", infer_nrows=150, header=None, names=[\"Twitter\"])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def regularize(text, tk):\n",
    "    tokens = tk.tokenize(text)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.find(\"http\") != -1:\n",
    "            token = re.sub(r\"http://\", \"{\",token)\n",
    "            token = re.sub(r\"/[\\w./-]+\", \"}\", token)\n",
    "            tokens[i]=token\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stemming(tokens, stemmer):\n",
    "    return [stemmer.stem(token).lower() for token in tokens]\n",
    "\n",
    "\n",
    "def pre_processing(df, description):\n",
    "    stemmed=[]\n",
    "    tk = TweetTokenizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    print(\"Start normalizing {} set\".format(description))\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        tokens = regularize(row[\"Twitter\"], tk)\n",
    "        stemmed.append(\"\\x01\".join(stemming(tokens, stemmer)))\n",
    "    del df[\"Twitter\"]\n",
    "    df[\"Twitter\"] = stemmed\n",
    "    df.to_csv(data_folder + \"{}_norm.csv\".format(description),index=False)\n",
    "    return df\n",
    "\n",
    "# Specify data folder\n",
    "data_folder = \"./\"\n",
    "\n",
    "train, test = read_data()\n",
    "if check_file_exist(data_folder+\"train_norm.csv\"):\n",
    "    print(\"tain_norm.txt exists. Will load it\")\n",
    "    train_norm = pd.read_csv(data_folder+\"train_norm.csv\")\n",
    "else:\n",
    "    train_norm = pre_processing(train, \"train\")\n",
    "if check_file_exist(data_folder+\"test_norm.csv\"):\n",
    "    print(\"test_norm.txt exists. Will load it\")\n",
    "    test_norm = pd.read_csv(data_folder+\"test_norm.csv\")\n",
    "else:\n",
    "    test_norm = pre_processing(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize data using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_data(feature_extractor):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    training_classifications = []\n",
    "    for _, row in tqdm(train_norm.iterrows(),total = train_norm.shape[0]):\n",
    "        feature_dict = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))   \n",
    "        training_set.append(feature_dict)\n",
    "        training_classifications.append(row[\"UID\"])\n",
    "    for _, row in tqdm(test_norm.iterrows(),total = test_norm.shape[0]):\n",
    "        features = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))\n",
    "        test_set.append(features)\n",
    "    vectorizer = DictVectorizer()\n",
    "    training_data = vectorizer.fit_transform(training_set)\n",
    "    test_data = vectorizer.transform(test_set)\n",
    "    return training_data,training_classifications,test_data\n",
    "\n",
    "# trn_data,trn_classes,test_data = prepare_data(get_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize data using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196d62fc91cd4e8ea645ca86a8d1d58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=328931), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa5ceccf72c46878b371bad91588074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35437), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def prepare_data_tfidf(feature_extractor):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    training_classifications = []\n",
    "    for _, row in tqdm(train_norm.iterrows(),total = train_norm.shape[0]):\n",
    "        feature_dict = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))   \n",
    "        training_set.append(feature_dict)\n",
    "        training_classifications.append(row[\"UID\"])\n",
    "\n",
    "    for _, row in tqdm(test_norm.iterrows(),total = test_norm.shape[0]):\n",
    "        features = feature_extractor(row[\"Twitter\"].split(\"\\x01\"))\n",
    "        test_set.append(features)\n",
    "        \n",
    "    vectorizer = DictVectorizer()\n",
    "    transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "    \n",
    "    training_data = transformer.fit_transform(vectorizer.fit_transform(training_set))\n",
    "    test_data  = transformer.transform(vectorizer.transform(test_set))\n",
    "    return training_data,training_classifications,test_data\n",
    "\n",
    "trn_data,trn_classes,test_data = prepare_data_tfidf(get_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [KNeighborsClassifier(n_jobs=-1),DecisionTreeClassifier(),RandomForestClassifier(n_jobs=-1, n_estimators=50),\n",
    "        MultinomialNB(),LinearSVC(verbose=10),LogisticRegression(n_jobs=-1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just for comparing the perforamnce of different models\n",
    "High memory requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    stacking_train_data=[]\n",
    "    for clf in clfs:\n",
    "        s_time = time()\n",
    "        predictions = model_selection.cross_val_predict(clf, data, classifications, cv=10)\n",
    "        stacking_train_data.append(predictions)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        print(\"time cost:{}\".format(time() - s_time))\n",
    "    return np.transpose(stacking_train_data)\n",
    "\n",
    "def get_cv_predictions(begin_idx, end_idx):\n",
    "    if check_file_exist(data_folder+\"cv_predictions_{}-{}.csv\".format(begin_idx, end_idx-1)):\n",
    "        print(\"cv_predictions_{}-{}.csv\".format(begin_idx, end_idx-1)+\" exists. Will load it\")\n",
    "        stacking_train_data = pd.read_csv(data_folder+\"cv_predictions_{}-{}.csv\".format(begin_idx, end_idx-1))[[str(i) for i in range(begin_idx,end_idx)]].values\n",
    "    else:\n",
    "        stacking_train_data = do_multiple_10foldcrossvalidation(clfs[begin_idx:end_idx],trn_data,trn_classes)\n",
    "        out_df = pd.DataFrame(stacking_train_data, columns=[i for i in range(begin_idx,end_idx)])\n",
    "        out_df.to_csv(data_folder + \"cv_predictions_{}-{}.csv\".format(begin_idx, end_idx-1))\n",
    "    return stacking_train_data\n",
    "\n",
    "get_cv_predictions(4,5) #get the cross validation prediction for model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save_predicted(predicted, index):\n",
    "    output = [(i+1,pred) for i,pred in enumerate(predicted)]\n",
    "    out_df = pd.DataFrame(output, columns=[\"Id\", \"Predicted\"]).set_index(\"Id\")\n",
    "    out_df.to_csv(data_folder + \"predicted_{}.csv\".format(index))\n",
    "\n",
    "def fit_predict(clfs, indexs, data, classifications):\n",
    "    for i in indexs:\n",
    "        s_time = time()\n",
    "        clfs[i].fit(data, classifications)\n",
    "        save_predicted(clfs[i].predict(test_data), i)\n",
    "        with open('trained_{}.pkl'.format(i), 'wb') as fid:\n",
    "            pickle.dump(clfs[i], fid, protocol=4)\n",
    "        print(\"time cost:{}\".format(time() - s_time))\n",
    "    \n",
    "def load_predict(indexs):\n",
    "    for i in indexs:\n",
    "        with open('trained_{}.pkl'.format(i), 'rb') as fid:\n",
    "            model = pickle.load(fid)\n",
    "        save_predicted(model.predict(test_data),i)\n",
    "\n",
    "models_index = list(range(4,5))\n",
    "fit_predict(clfs, models_index, trn_data, trn_classes)\n",
    "# load_predict(models_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ensemble with xgboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_predictions_4-5.csv exists. Will load it\n",
      "time cost:0.08347606658935547\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def load_xgboosting_test_data(begin_idx, end_idx):\n",
    "    x_test=[]\n",
    "    for i in range(begin_idx, end_idx):\n",
    "        x_test.append(np.array(pd.read_csv('predicted_{}.csv'.format(i))[\"Predicted\"]))\n",
    "    return np.transpose(x_test)\n",
    "        \n",
    "def xgboosting_fit_predict(begin_idx, end_idx):\n",
    "    s_time = time()\n",
    "    x_train=get_cv_predictions(begin_idx, end_idx)\n",
    "    gbm = xgb.XGBClassifier(\n",
    "         n_estimators= 2000,\n",
    "        #  max_depth= 4,\n",
    "         min_child_weight= 1,\n",
    "         gamma=0.5,                        \n",
    "         subsample=1,\n",
    "         colsample_bytree=1,\n",
    "         objective= 'multi:softmax',\n",
    "         nthread= -1,\n",
    "         scale_pos_weight=1).fit(x_train, trn_classes)\n",
    "    \n",
    "    x_test = load_xgboosting_test_data(begin_idx, end_idx)\n",
    "    save_predicted(gbm.predict(x_test), \"gbm_{}-{}\".format(begin_idx, end_idx))\n",
    "    \n",
    "    with open('trained_xgboosting_{}-{}.pkl'.format(begin_idx, end_idx-1), 'wb') as fid:\n",
    "        pickle.dump(gbm, fid, protocol=4)\n",
    "    print(\"time cost:{}\".format(time() - s_time))\n",
    "\n",
    "def xgboosting_load_predict(begin_idx, end_idx):\n",
    "    with open('trained_xgboosting_{}-{}.pkl'.format(begin_idx, end_idx-1), 'rb') as fid:\n",
    "        model = pickle.load(fid)\n",
    "    x_test = load_xgboosting_test_data(begin_idx, end_idx)\n",
    "    save_predicted(model.predict(x_test),\"gbm_{}-{}\".format(begin_idx, end_idx-1))\n",
    "    \n",
    "xgboosting_fit_predict(4, 6)\n",
    "# xgboosting_load_predict(4, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
